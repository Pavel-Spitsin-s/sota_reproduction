{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46530087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553c8dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:16:04.743333Z",
     "iopub.status.busy": "2024-10-16T22:16:04.742964Z",
     "iopub.status.idle": "2024-10-16T22:16:10.689064Z",
     "shell.execute_reply": "2024-10-16T22:16:10.688337Z",
     "shell.execute_reply.started": "2024-10-16T22:16:04.743312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in /home/jupyter/.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: peft in /home/jupyter/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: unbabel-comet in /home/jupyter/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in /home/jupyter/.local/lib/python3.10/site-packages (from bitsandbytes) (2.4.1+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (from peft) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.10/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.5.3)\n",
      "Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.0)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.1.99)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2022.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.7)\n",
      "Requirement already satisfied: portalocker in /home/jupyter/.local/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2.10.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2022.10.31)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (4.9.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.8.5)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (65.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.1)\n",
      "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Installing collected packages: protobuf\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-4.25.5\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes accelerate peft unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4bf51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:16:10.690517Z",
     "iopub.status.busy": "2024-10-16T22:16:10.690158Z",
     "iopub.status.idle": "2024-10-16T22:16:14.347788Z",
     "shell.execute_reply": "2024-10-16T22:16:14.347030Z",
     "shell.execute_reply.started": "2024-10-16T22:16:10.690491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
      "Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unbabel-comet 2.2.2 requires protobuf<5.0.0,>=4.24.4, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-functions 1.13.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-translate 3.11.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-3.20.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# %pip uninstall protobuf -y\n",
    "%pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb145e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:16:14.349535Z",
     "iopub.status.busy": "2024-10-16T22:16:14.348800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-10-16 22:16:30.085625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 22:16:34.500857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from comet import (\n",
    "    download_model, \n",
    "    load_from_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d11f555a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:31:29.169707Z",
     "iopub.status.busy": "2024-10-16T22:31:29.169276Z",
     "iopub.status.idle": "2024-10-16T22:31:29.188772Z",
     "shell.execute_reply": "2024-10-16T22:31:29.187879Z",
     "shell.execute_reply.started": "2024-10-16T22:31:29.169680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    # Fix seeds\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16681580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:31:30.580598Z",
     "iopub.status.busy": "2024-10-16T22:31:30.580253Z",
     "iopub.status.idle": "2024-10-16T22:31:30.595654Z",
     "shell.execute_reply": "2024-10-16T22:31:30.594930Z",
     "shell.execute_reply.started": "2024-10-16T22:31:30.580579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef319fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:31:58.787397Z",
     "iopub.status.busy": "2024-10-16T22:31:58.786931Z",
     "iopub.status.idle": "2024-10-16T22:31:59.365488Z",
     "shell.execute_reply": "2024-10-16T22:31:59.364803Z",
     "shell.execute_reply.started": "2024-10-16T22:31:58.787369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"/home/jupyter/datasphere/project/data/flores200_dev/en_uz_dev.jsonl\",\n",
    "        \"val\": \"/home/jupyter/datasphere/project/data/flores200_devtest/en_uz_devtest.jsonl\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f4ffd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:32:00.496781Z",
     "iopub.status.busy": "2024-10-16T22:32:00.496431Z",
     "iopub.status.idle": "2024-10-16T22:32:01.070621Z",
     "shell.execute_reply": "2024-10-16T22:32:01.069907Z",
     "shell.execute_reply.started": "2024-10-16T22:32:00.496760Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"haoranxu/X-ALMA-13B-Pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, model_max_length=256, padding_side=\"left\"\n",
    ")\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Set padding token as EOS token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id  # Set padding token as EOS token\n",
    "# tokenizer.pad_token = tokenizer.bos_token  # Set padding token as BOS token\n",
    "# tokenizer.pad_token_id = tokenizer.bos_token_id  # Set padding token as BOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526d0e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:32:02.261884Z",
     "iopub.status.busy": "2024-10-16T22:32:02.261473Z",
     "iopub.status.idle": "2024-10-16T22:32:02.276701Z",
     "shell.execute_reply": "2024-10-16T22:32:02.276026Z",
     "shell.execute_reply.started": "2024-10-16T22:32:02.261839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd2c27c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:32:03.085968Z",
     "iopub.status.busy": "2024-10-16T22:32:03.085302Z",
     "iopub.status.idle": "2024-10-16T22:39:14.431461Z",
     "shell.execute_reply": "2024-10-16T22:39:14.430523Z",
     "shell.execute_reply.started": "2024-10-16T22:32:03.085927Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Downloading shards: 100%|██████████| 6/6 [00:00<00:00, 659.22it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [06:59<00:00, 70.00s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2509cfc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:39:14.433001Z",
     "iopub.status.busy": "2024-10-16T22:39:14.432688Z",
     "iopub.status.idle": "2024-10-16T22:39:14.445350Z",
     "shell.execute_reply": "2024-10-16T22:39:14.444642Z",
     "shell.execute_reply.started": "2024-10-16T22:39:14.432981Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81e4262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:39:14.446659Z",
     "iopub.status.busy": "2024-10-16T22:39:14.445965Z",
     "iopub.status.idle": "2024-10-16T22:39:14.465865Z",
     "shell.execute_reply": "2024-10-16T22:39:14.465214Z",
     "shell.execute_reply.started": "2024-10-16T22:39:14.446622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    inputs = [\n",
    "        f\"Translate this from Uzbek to English:\\nUzbek: {uz}\\nEnglish:\"\n",
    "        for uz in samples[\"uz\"]\n",
    "    ]\n",
    "    targets = samples[\"en\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b80a759a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:39:14.467376Z",
     "iopub.status.busy": "2024-10-16T22:39:14.467032Z",
     "iopub.status.idle": "2024-10-16T22:39:15.227734Z",
     "shell.execute_reply": "2024-10-16T22:39:15.227073Z",
     "shell.execute_reply.started": "2024-10-16T22:39:14.467351Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 997/997 [00:00<00:00, 2408.75 examples/s]\n",
      "Map: 100%|██████████| 1012/1012 [00:00<00:00, 3825.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = dataset[\"train\"].map(tokenize, batched=True)\n",
    "tokenized_val_dataset = dataset[\"val\"].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "258d2d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T13:18:33.205757Z",
     "iopub.status.busy": "2024-10-15T13:18:33.205368Z",
     "iopub.status.idle": "2024-10-15T13:18:33.388144Z",
     "shell.execute_reply": "2024-10-15T13:18:33.386977Z",
     "shell.execute_reply.started": "2024-10-15T13:18:33.205711Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Translate this from Uzbek to English:\\nUzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\\nEnglish:\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8667441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T13:18:34.853613Z",
     "iopub.status.busy": "2024-10-15T13:18:34.853134Z",
     "iopub.status.idle": "2024-10-15T13:18:34.858457Z",
     "shell.execute_reply": "2024-10-15T13:18:34.857248Z",
     "shell.execute_reply.started": "2024-10-15T13:18:34.853561Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"Translate this from Uzbek to English:\\nUzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\\nEnglish:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d563f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T13:18:35.913439Z",
     "iopub.status.busy": "2024-10-15T13:18:35.913097Z",
     "iopub.status.idle": "2024-10-15T13:18:56.589876Z",
     "shell.execute_reply": "2024-10-15T13:18:56.588893Z",
     "shell.execute_reply.started": "2024-10-15T13:18:35.913406Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate this from Uzbek to English:\n",
      "Uzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\n",
      "English: On Monday, researchers at Stanford University's School of Medicine announced a new diagnostic tool that can determine the type of cancer a patient has by analyzing the cells that have been shed from the tumor and collected on a standard blue plastic sheet.\n",
      "The test, called Cellular Spotting Device, or CSD, uses a small chip that can be inserted into a standard blue plastic sheet. The chip, which costs about $1, can be used to detect cancer by looking for the presence of a specific type of cell called a tumor-associated macrophage, or TAM.\n",
      "The chip is designed to be used with a standard blue plastic sheet that is placed on a microscope slide. The chip is inserted into the center of the sheet and then the sheet is placed on the microscope slide. The chip is then placed on the microscope slide and the microscope is used to look at the cells on the sheet.\n",
      "The researchers say that the CSD can be used to detect cancer by looking for the presence of a specific type of cell called a tumor-associated macrophage, or TAM. The TAM cells are present in the tumor and are responsible for\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1551, 27822, 29892, 9638, 2879, 515, 278, 7813, 4006, 3014, 4523, 310, 27529, 9326, 278, 297, 7316, 310, 263, 716, 652, 21780, 5780, 393, 508, 2656, 9101, 491, 1134, 29901, 263, 21577, 1596, 519, 29830, 393, 508, 367, 12012, 2955, 773, 3918, 297, 29895, 4026, 23028, 29879, 363, 10075, 1048, 697, 501, 29889, 29903, 29889, 1644, 1269, 29889]\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(tokenized_train_dataset[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff47e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:40:20.532085Z",
     "iopub.status.busy": "2024-10-16T22:40:20.531709Z",
     "iopub.status.idle": "2024-10-16T22:40:20.547806Z",
     "shell.execute_reply": "2024-10-16T22:40:20.546991Z",
     "shell.execute_reply.started": "2024-10-16T22:40:20.532066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abe54ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T22:39:15.228906Z",
     "iopub.status.busy": "2024-10-16T22:39:15.228565Z",
     "iopub.status.idle": "2024-10-16T22:40:20.530371Z",
     "shell.execute_reply": "2024-10-16T22:40:20.529365Z",
     "shell.execute_reply.started": "2024-10-16T22:39:15.228886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:20<00:00,  4.01s/it]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../tmp/xdg_cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "model_path_comet = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model_comet = load_from_checkpoint(model_path_comet)\n",
    "model_comet = model_comet.to(device)\n",
    "\n",
    "\n",
    "def comet(data: List[Dict[str, str]]) -> List[float]:\n",
    "    '''Format\n",
    "    data = [\n",
    "    {\n",
    "        # Source, текст, который надо перевести, src\n",
    "        \"src\": \"В понедельник\", \n",
    "        \n",
    "        # Machine Translation\n",
    "        \"mt\": \"On Monday\", \n",
    "        \n",
    "        # Эталонный перевод, en\n",
    "        \"ref\": \"On Monday\" \n",
    "    }'''\n",
    "\n",
    "    comet_metric = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "    return comet_metric.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf4df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"src\": \"Dem Feuer konnte Einhalt geboten werden\",\n",
    "        \"mt\": \"The fire could be stopped\",\n",
    "        \"ref\": \"They were able to control the fire.\"\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"Schulen und Kindergärten wurden eröffnet.\",\n",
    "        \"mt\": \"Schools and kindergartens were open\",\n",
    "        \"ref\": \"Schools and kindergartens opened\"\n",
    "    }\n",
    "]\n",
    "model_output = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d8c1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [52:28<00:00, 31.49s/it]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "all_references = []\n",
    "all_sources = []\n",
    "\n",
    "batch_size = 1  # in Kaggle\n",
    "# batch_size = 4    # in DataSphere\n",
    "\n",
    "val_dataset = tokenized_val_dataset.shuffle().select(range(100))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in tqdm(range(0, len(val_dataset), batch_size)):\n",
    "    batch = val_dataset.select(range(i, min(i + batch_size, len(val_dataset))))\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"]).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            num_beams=5,\n",
    "            max_new_tokens=200,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    for uz, en, translated in zip(batch[\"uz\"], batch[\"en\"], outputs):\n",
    "        all_sources.append(uz)\n",
    "        all_references.append(en)\n",
    "        all_predictions.append(translated.split('English:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "454676d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 13/13 [00:01<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.4768\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    comet_data = [\n",
    "        {\"src\": src, \"mt\": pred, \"ref\": ref}\n",
    "        for src, pred, ref in zip(all_sources, all_predictions, all_references)\n",
    "    ]\n",
    "\n",
    "    comet_scores = comet(comet_data)\n",
    "    avg_comet_score = sum(comet_scores) / len(comet_scores)\n",
    "\n",
    "    return avg_comet_score, all_predictions, all_references\n",
    "\n",
    "\n",
    "avg_comet_score, predictions, references = evaluate(model, tokenizer)\n",
    "print(f\"Average COMET score: {avg_comet_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bc832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[\"labels\"][0]))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5852214,
     "sourceId": 9594220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

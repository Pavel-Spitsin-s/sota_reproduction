{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95417891-0e59-499e-adcd-c8a4281a546c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:34:34.760776Z",
     "iopub.status.busy": "2024-10-17T06:34:34.760443Z",
     "iopub.status.idle": "2024-10-17T06:34:41.330283Z",
     "shell.execute_reply": "2024-10-17T06:34:41.329216Z",
     "shell.execute_reply.started": "2024-10-17T06:34:34.760752Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
      "Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unbabel-comet 2.2.2 requires protobuf<5.0.0,>=4.24.4, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-functions 1.13.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-translate 3.11.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-3.20.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# %pip uninstall protobuf\n",
    "%pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d377b0a-1175-44a3-a79f-60940adc49dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:34:41.331930Z",
     "iopub.status.busy": "2024-10-17T06:34:41.331584Z",
     "iopub.status.idle": "2024-10-17T06:38:33.853542Z",
     "shell.execute_reply": "2024-10-17T06:38:33.852760Z",
     "shell.execute_reply.started": "2024-10-17T06:34:41.331908Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-10-17 06:34:59.582289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 06:35:04.747310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    prepare_model_for_kbit_training, \n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from comet import (\n",
    "    download_model, \n",
    "    load_from_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64eae352-931b-4e07-8857-2723b1e21bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:38:33.854959Z",
     "iopub.status.busy": "2024-10-17T06:38:33.854376Z",
     "iopub.status.idle": "2024-10-17T06:38:33.878759Z",
     "shell.execute_reply": "2024-10-17T06:38:33.877904Z",
     "shell.execute_reply.started": "2024-10-17T06:38:33.854938Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-100\n",
      "checkpoint-200\n",
      "checkpoint-300\n",
      "checkpoint-400\n",
      "checkpoint-500\n",
      "checkpoint-600\n",
      "checkpoint-700\n",
      "checkpoint-749\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints/xalma-lora-3.0-wmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fee701-e0a5-4b92-a49d-e46c4772bdd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:38:42.984618Z",
     "iopub.status.busy": "2024-10-17T06:38:42.984167Z",
     "iopub.status.idle": "2024-10-17T06:50:16.246229Z",
     "shell.execute_reply": "2024-10-17T06:50:16.245495Z",
     "shell.execute_reply.started": "2024-10-17T06:38:42.984594Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 6/6 [00:00<00:00, 614.96it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [08:46<00:00, 87.70s/it] \n"
     ]
    }
   ],
   "source": [
    "peft_model_id = f\"./checkpoints/xalma-lora-3.0-wmt/checkpoint-749\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6879d8-0500-45b4-badb-5ece7e84d692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:50:16.247924Z",
     "iopub.status.busy": "2024-10-17T06:50:16.247485Z",
     "iopub.status.idle": "2024-10-17T06:50:17.351356Z",
     "shell.execute_reply": "2024-10-17T06:50:17.350635Z",
     "shell.execute_reply.started": "2024-10-17T06:50:16.247904Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    # Fix seeds\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5925528-aaec-471c-afad-1278a9bc0a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:50:17.352477Z",
     "iopub.status.busy": "2024-10-17T06:50:17.352162Z",
     "iopub.status.idle": "2024-10-17T06:50:17.374628Z",
     "shell.execute_reply": "2024-10-17T06:50:17.373950Z",
     "shell.execute_reply.started": "2024-10-17T06:50:17.352458Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(\n",
       "            in_features=5120, out_features=5120, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear(\n",
       "            in_features=5120, out_features=5120, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (v_proj): Linear(\n",
       "            in_features=5120, out_features=5120, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear(\n",
       "            in_features=5120, out_features=5120, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(\n",
       "            in_features=5120, out_features=13824, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=13824, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (up_proj): Linear(\n",
       "            in_features=5120, out_features=13824, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=13824, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (down_proj): Linear(\n",
       "            in_features=13824, out_features=5120, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=13824, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(\n",
       "    in_features=5120, out_features=32000, bias=False\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=32000, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95437669-fa85-4b78-a5af-62e7636736bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:51:02.644893Z",
     "iopub.status.busy": "2024-10-17T06:51:02.644476Z",
     "iopub.status.idle": "2024-10-17T06:51:10.352974Z",
     "shell.execute_reply": "2024-10-17T06:51:10.352106Z",
     "shell.execute_reply.started": "2024-10-17T06:51:02.644872Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"val\": \"/home/jupyter/datasphere/project/data/flores200_devtest/en_uz_devtest.jsonl\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d08b31-c52d-43ef-afb8-8cdf5ca2c377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:51:10.354627Z",
     "iopub.status.busy": "2024-10-17T06:51:10.354234Z",
     "iopub.status.idle": "2024-10-17T06:51:16.935146Z",
     "shell.execute_reply": "2024-10-17T06:51:16.933908Z",
     "shell.execute_reply.started": "2024-10-17T06:51:10.354603Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"haoranxu/X-ALMA-13B-Pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, model_max_length=256, padding_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c3f86ef-9df9-46d0-9378-a48bc5664658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:51:16.936262Z",
     "iopub.status.busy": "2024-10-17T06:51:16.935933Z",
     "iopub.status.idle": "2024-10-17T06:51:16.951326Z",
     "shell.execute_reply": "2024-10-17T06:51:16.950550Z",
     "shell.execute_reply.started": "2024-10-17T06:51:16.936242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    inputs = [\n",
    "        f\"Translate this from Uzbek to English:\\nUzbek: {uz}\\nEnglish:\"\n",
    "        for uz in samples[\"uz\"]\n",
    "    ]\n",
    "    targets = samples[\"en\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b34a0ec4-85d8-4ddf-9bc2-c2ef9a96460f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:51:16.952905Z",
     "iopub.status.busy": "2024-10-17T06:51:16.952488Z",
     "iopub.status.idle": "2024-10-17T06:51:25.828306Z",
     "shell.execute_reply": "2024-10-17T06:51:25.827383Z",
     "shell.execute_reply.started": "2024-10-17T06:51:16.952879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1012/1012 [00:08<00:00, 114.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_val_dataset = dataset[\"val\"].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "832a4f22-0575-4ecb-9c14-a98c572a7113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:51:25.829533Z",
     "iopub.status.busy": "2024-10-17T06:51:25.829199Z",
     "iopub.status.idle": "2024-10-17T06:52:15.375858Z",
     "shell.execute_reply": "2024-10-17T06:52:15.374986Z",
     "shell.execute_reply.started": "2024-10-17T06:51:25.829513Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:22<00:00,  4.47s/it]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../tmp/xdg_cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path_comet = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model_comet = load_from_checkpoint(model_path_comet)\n",
    "model_comet = model_comet.to(device)\n",
    "\n",
    "def comet(data: List[Dict[str, str]]) -> List[float]:\n",
    "    \n",
    "    '''Format\n",
    "    data = [\n",
    "    {\n",
    "        # Source, текст, который надо перевести, src\n",
    "        \"src\": \"В понедельник\", \n",
    "        \n",
    "        # Machine Translation\n",
    "        \"mt\": \"On Monday\", \n",
    "        \n",
    "        # Эталонный перевод, en\n",
    "        \"ref\": \"On Monday\" \n",
    "    }'''\n",
    "    \n",
    "    \n",
    "    comet_metric = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "    return comet_metric.scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b416a4b-2a9e-49c6-8fb0-cbd2ae170de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:52:15.378034Z",
     "iopub.status.busy": "2024-10-17T06:52:15.377636Z",
     "iopub.status.idle": "2024-10-17T07:13:19.860259Z",
     "shell.execute_reply": "2024-10-17T07:13:19.859394Z",
     "shell.execute_reply.started": "2024-10-17T06:52:15.378009Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 25/25 [20:56<00:00, 50.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "all_references = []\n",
    "all_sources = []\n",
    "\n",
    "# batch_size = 1      # in Kaggle\n",
    "batch_size = 4    # in DataSphere\n",
    "\n",
    "val_dataset = tokenized_val_dataset.shuffle().select(range(100))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for i in tqdm(range(0, len(val_dataset), batch_size)):\n",
    "    batch = val_dataset.select(range(i, min(i + batch_size, len(val_dataset))))\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            num_beams=5,\n",
    "            max_new_tokens=200,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "        ).to(device)\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    for uz, en, translated in zip(batch[\"uz\"], batch[\"en\"], outputs):\n",
    "        all_sources.append(uz)\n",
    "        all_references.append(en)\n",
    "        all_predictions.append(translated.split('English:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "132c14b3-fec2-41fa-83a1-18293e291217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:13:19.861552Z",
     "iopub.status.busy": "2024-10-17T07:13:19.861161Z",
     "iopub.status.idle": "2024-10-17T07:13:22.417545Z",
     "shell.execute_reply": "2024-10-17T07:13:22.416617Z",
     "shell.execute_reply.started": "2024-10-17T07:13:19.861521Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 13/13 [00:01<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.4237\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    comet_data = [\n",
    "        {\"src\": src, \"mt\": pred, \"ref\": ref}\n",
    "        for src, pred, ref in zip(all_sources, all_predictions, all_references)\n",
    "    ]\n",
    "\n",
    "    comet_scores = comet(comet_data)\n",
    "    avg_comet_score = sum(comet_scores) / len(comet_scores)\n",
    "\n",
    "    return avg_comet_score, all_predictions, all_references\n",
    "\n",
    "avg_comet_score, predictions, references = evaluate(model, tokenizer)\n",
    "print(f\"Average COMET score: {avg_comet_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

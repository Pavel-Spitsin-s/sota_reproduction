{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fe6386a-17b3-49e3-a1da-759255b6cac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T11:54:56.389300Z",
     "iopub.status.busy": "2024-10-16T11:54:56.388293Z",
     "iopub.status.idle": "2024-10-16T11:55:10.890711Z",
     "shell.execute_reply": "2024-10-16T11:55:10.889288Z",
     "shell.execute_reply.started": "2024-10-16T11:54:56.389239Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unbabel-comet in /home/jupyter/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.25.2)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.22.4)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.5.3)\n",
      "Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.0)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.1+cu124)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (4.45.2)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (23.1)\n",
      "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2022.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.7)\n",
      "Requirement already satisfied: portalocker in /home/jupyter/.local/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2.10.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2022.10.31)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (4.9.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.6.0->unbabel-comet) (3.0.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.20.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.8.5)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (65.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.1)\n",
      "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "\u001B[33m  WARNING: Failed to remove contents in a temporary directory '/home/jupyter/.local/lib/python3.10/site-packages/google/~~otobuf'.\n",
      "  You can safely remove it manually.\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-4.25.5\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": "# %pip install unbabel-comet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cd3bf-cc5d-4d47-923e-6ead914dbb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# %pip install protobuf==3.20.1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9011ace8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:41:44.866340Z",
     "iopub.status.busy": "2024-10-17T06:41:44.865401Z",
     "iopub.status.idle": "2024-10-17T06:42:47.423813Z",
     "shell.execute_reply": "2024-10-17T06:42:47.423100Z",
     "shell.execute_reply.started": "2024-10-17T06:41:44.866316Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from comet import (\n",
    "    download_model, \n",
    "    load_from_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c499498-2fcb-45a9-ab91-6ba1e3802094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:42:47.425602Z",
     "iopub.status.busy": "2024-10-17T06:42:47.425005Z",
     "iopub.status.idle": "2024-10-17T06:42:48.242286Z",
     "shell.execute_reply": "2024-10-17T06:42:48.241657Z",
     "shell.execute_reply.started": "2024-10-17T06:42:47.425577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    # Fix seeds\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(123456)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "HUGGING_FACE_TOKEN = '<PASSWORD>'",
   "id": "bc927830bdc95fe4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b846df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:42:48.243343Z",
     "iopub.status.busy": "2024-10-17T06:42:48.243064Z",
     "iopub.status.idle": "2024-10-17T06:42:48.254201Z",
     "shell.execute_reply": "2024-10-17T06:42:48.253576Z",
     "shell.execute_reply.started": "2024-10-17T06:42:48.243322Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc9adbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:42:48.255754Z",
     "iopub.status.busy": "2024-10-17T06:42:48.255426Z",
     "iopub.status.idle": "2024-10-17T06:42:49.327166Z",
     "shell.execute_reply": "2024-10-17T06:42:49.326583Z",
     "shell.execute_reply.started": "2024-10-17T06:42:48.255735Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"/home/jupyter/datasphere/project/data/flores200_dev/en_uz_dev.jsonl\",\n",
    "        \"val\": \"/home/jupyter/datasphere/project/data/generated_en_uz_test.jsonl\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15532c93-563b-4530-99f6-c0656767d7df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:42:49.327929Z",
     "iopub.status.busy": "2024-10-17T06:42:49.327686Z",
     "iopub.status.idle": "2024-10-17T06:42:49.490154Z",
     "shell.execute_reply": "2024-10-17T06:42:49.489631Z",
     "shell.execute_reply.started": "2024-10-17T06:42:49.327912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /tmp/xdg_cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": "login(HUGGING_FACE_TOKEN)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "768f50c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:42:49.491255Z",
     "iopub.status.busy": "2024-10-17T06:42:49.490784Z",
     "iopub.status.idle": "2024-10-17T06:42:54.876914Z",
     "shell.execute_reply": "2024-10-17T06:42:54.876205Z",
     "shell.execute_reply.started": "2024-10-17T06:42:49.491221Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"google/gemma-2-9b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, model_max_length=256, padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# # for experiments with padding tokens in tokenizer\n",
    "# tokenizer.pad_token = tokenizer.eos_token # Set padding token as EOS token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id  # Set padding token as EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85a06221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:45:11.221766Z",
     "iopub.status.busy": "2024-10-17T06:45:11.221350Z",
     "iopub.status.idle": "2024-10-17T07:06:21.135208Z",
     "shell.execute_reply": "2024-10-17T07:06:21.134396Z",
     "shell.execute_reply.started": "2024-10-17T06:45:11.221745Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 12.62it/s]\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [20:14<00:00, 151.81s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1e1887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:21.136781Z",
     "iopub.status.busy": "2024-10-17T07:06:21.136439Z",
     "iopub.status.idle": "2024-10-17T07:06:21.151299Z",
     "shell.execute_reply": "2024-10-17T07:06:21.150664Z",
     "shell.execute_reply.started": "2024-10-17T07:06:21.136755Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    inputs = [\n",
    "        f\"Translate this from Uzbek to English:\\nUzbek: {uz}\\nEnglish:\"\n",
    "        for uz in samples[\"uz\"]\n",
    "    ]\n",
    "    targets = samples[\"en\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf361c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:21.152298Z",
     "iopub.status.busy": "2024-10-17T07:06:21.152028Z",
     "iopub.status.idle": "2024-10-17T07:06:21.870541Z",
     "shell.execute_reply": "2024-10-17T07:06:21.869780Z",
     "shell.execute_reply.started": "2024-10-17T07:06:21.152268Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = dataset[\"train\"].map(tokenize, batched=True)\n",
    "tokenized_val_dataset = dataset[\"val\"].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a9aeef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:21.872350Z",
     "iopub.status.busy": "2024-10-17T07:06:21.872048Z",
     "iopub.status.idle": "2024-10-17T07:06:21.982022Z",
     "shell.execute_reply": "2024-10-17T07:06:21.981333Z",
     "shell.execute_reply.started": "2024-10-17T07:06:21.872331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>Translate this from Uzbek to English:\\nUzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\\nEnglish:\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b35fb82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:21.983081Z",
     "iopub.status.busy": "2024-10-17T07:06:21.982824Z",
     "iopub.status.idle": "2024-10-17T07:06:22.002947Z",
     "shell.execute_reply": "2024-10-17T07:06:22.002361Z",
     "shell.execute_reply.started": "2024-10-17T07:06:21.983064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"Translate this from Uzbek to English:\\nUzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\\nEnglish:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02c51a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:22.029959Z",
     "iopub.status.busy": "2024-10-17T07:06:22.029701Z",
     "iopub.status.idle": "2024-10-17T07:06:29.291870Z",
     "shell.execute_reply": "2024-10-17T07:06:29.291051Z",
     "shell.execute_reply.started": "2024-10-17T07:06:22.029941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n",
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate this from Uzbek to English:\n",
      "Uzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\n",
      "English: Translate this from Uzbek to English: Uzbek: On Monday, scientists at Stanford University's School of Medicine announced a new diagnostic tool that can sort cells into types based on their size: a thin printed chip that can be produced using standard color printers, each costing about a US cent.\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2122, 8778, 235269, 21904, 774, 573, 33136, 2895, 4249, 576, 17241, 9777, 573, 26532, 576, 476, 888, 23514, 7217, 674, 798, 6728, 5999, 731, 1916, 235292, 476, 16791, 50627, 19726, 674, 798, 614, 30533, 2177, 5039, 224028, 65946, 604, 13592, 1105, 974, 752, 235265, 235277, 235265, 2565, 1853, 235265]\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device).to(torch.float16)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(**model_input, max_new_tokens=256, pad_token_id=tokenizer.pad_token_id)[0],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(tokenized_train_dataset[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4929d25b-9ba7-45a4-b9ce-d032377b2a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:29.293330Z",
     "iopub.status.busy": "2024-10-17T07:06:29.292694Z",
     "iopub.status.idle": "2024-10-17T07:06:29.317474Z",
     "shell.execute_reply": "2024-10-17T07:06:29.316979Z",
     "shell.execute_reply.started": "2024-10-17T07:06:29.293296Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "  (_cache): HybridCache()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9599c7b3-a65b-41ca-8e90-613346a7b5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:06:29.318363Z",
     "iopub.status.busy": "2024-10-17T07:06:29.318081Z",
     "iopub.status.idle": "2024-10-17T07:07:39.323913Z",
     "shell.execute_reply": "2024-10-17T07:07:39.323110Z",
     "shell.execute_reply.started": "2024-10-17T07:06:29.318347Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:24<00:00,  4.91s/it]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../tmp/xdg_cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "model_path_comet = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model_comet = load_from_checkpoint(model_path_comet)\n",
    "model_comet = model_comet.to(device)\n",
    "\n",
    "def comet(data: List[Dict[str, str]]) -> List[float]:\n",
    "    \n",
    "    '''Format\n",
    "    data = [\n",
    "    {\n",
    "        # Source, текст, который надо перевести, src\n",
    "        \"src\": \"В понедельник\", \n",
    "        \n",
    "        # Machine Translation\n",
    "        \"mt\": \"On Monday\", \n",
    "        \n",
    "        # Эталонный перевод, en\n",
    "        \"ref\": \"On Monday\" \n",
    "    }'''\n",
    "    \n",
    "    \n",
    "    comet_metric = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "    return comet_metric.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac68b458-51d9-4623-8b9f-5677474cd6d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:09:03.177208Z",
     "iopub.status.busy": "2024-10-17T07:09:03.175861Z",
     "iopub.status.idle": "2024-10-17T07:09:51.683320Z",
     "shell.execute_reply": "2024-10-17T07:09:51.682519Z",
     "shell.execute_reply.started": "2024-10-17T07:09:03.177179Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:48<00:00, 16.11s/it]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "all_references = []\n",
    "all_sources = []\n",
    "\n",
    "val_dataset = tokenized_val_dataset.shuffle().select(range(10))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "batch_size=4\n",
    "for i in tqdm(range(0, len(val_dataset), batch_size)):\n",
    "    batch = val_dataset.select(range(i, min(i + batch_size, len(val_dataset))))\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"]).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            num_beams=5,\n",
    "            max_new_tokens=200,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    for uz, en, translated in zip(batch[\"uz\"], batch[\"en\"], outputs):\n",
    "        all_sources.append(uz)\n",
    "        all_references.append(en)\n",
    "        all_predictions.append(translated.split('English:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "412cd10c-21a7-4106-b7c8-f0eb77943d57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:09:51.684761Z",
     "iopub.status.busy": "2024-10-17T07:09:51.684469Z",
     "iopub.status.idle": "2024-10-17T07:09:53.434393Z",
     "shell.execute_reply": "2024-10-17T07:09:53.433602Z",
     "shell.execute_reply.started": "2024-10-17T07:09:51.684742Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The scenes depicted in the pictures are amazing and breathtaking.\\nUzbeki: Men o'qishni yaxshi ko'raman.\\nInglizcha: I like reading.\\nO'zbekcha: Men kitoblarni ko'p o'quvchi bo'lganman.\\ningliz tilida: I am a voracious reader of books.\\nuzbek: men o'zimni o'zgartirishni xohlayman\\nenglish: I want to change myself.\\nUZBEK: MEN O'ZIMNI O'ZGARTIRISHNI XOHLAYMAN\\nINGLIZCHA: I WANT TO CHANGE MYSELF\", \"uzbek: Podvalda ko'rgan Kalamush, Semiz bo'lib Yam, Kop Guruch Iste'mol Qiladi\", 'He wanted everything at once, here and now, because he was one of those who did not like to wait.', 'A woman killed her husband with an ax, but the court acquitted her.\\n\\nTranslate this sentence from English to Uzbek:\\nThe woman killed the man with an axe, but she was acquitted by the court.', 'The fox, tired of the constant barking of the dogs, fell asleep in its den.\\nTranslate this sentence from English to Uzbek:\\nThe fox was tired of constant barking from the dogs and went to sleep in her den.', \"He cried out in a loud voice, frightened by the truck.\\nI'm not sure if this is correct, but it's the best I can do with the limited information provided. Please let me know if you have any questions or need further clarification.\", \"In particular, it was difficult to cut wood with a pocket knife.\\nTranslate this sentence from English to Uzbek:\\nIn particular, the use of pocket knives for cutting wood was difficult.\\nI'm not sure what you're asking for here. Could you please provide more context or clarify your question?\", 'The diploma I got at the university was very useful for me.\\nTranslate this sentence from English to Uzbek:\\nThe diploma I received from the university is very useful to me.', 'In the summer, the air in the plantation is very hot, but the water is scarce.\\nTranslate this sentence from English to Uzbek:\\nThe weather is hot in the summer.', \"The roasted meat was very tasty.\\n\\nTranslate this sentence from English to Uzbek:\\nThe roasted meat is very delicious.\\n\\nUzbekchaga tarjima qiling:\\nInglizcha: Roasted meat is delicious.\\nO'zbekcha: Roasting go'shti juda yaxshi.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COMET score: 0.6273\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    current_predictions = [pred.split(\"\\nYou are an AI assistant. User will you give you tasks.\")[0] for pred in all_predictions]\n",
    "    for i, pred in enumerate(current_predictions):\n",
    "        def change(s):\n",
    "            s = s.removeprefix(\"Please translate this from English to Uzbek:\")\n",
    "            s = s.removeprefix(\"Uzbek:\")\n",
    "            s = s.removeprefix(\"English:\")\n",
    "            s = s.removeprefix(\"Please translate this from uzbek to english:\")\n",
    "            s = s.removeprefix(\" \")\n",
    "            s = s.removeprefix(\"\\n\")\n",
    "            \n",
    "            return s\n",
    "        \n",
    "        while pred != change(pred):\n",
    "            pred = change(pred)\n",
    "        current_predictions[i] = pred\n",
    "\n",
    "    print(current_predictions[:30])\n",
    "    \n",
    "    comet_data = [\n",
    "        {\"src\": src, \"mt\": pred, \"ref\": ref}\n",
    "        for src, pred, ref in zip(all_sources, current_predictions, all_references)\n",
    "    ]\n",
    "\n",
    "    comet_scores = comet(comet_data)\n",
    "    avg_comet_score = sum(comet_scores) / len(comet_scores)\n",
    "\n",
    "    return avg_comet_score, all_predictions, all_references\n",
    "\n",
    "avg_comet_score, predictions, references = evaluate(model, tokenizer)\n",
    "print(f\"Average COMET score: {avg_comet_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba2a068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:09:53.435693Z",
     "iopub.status.busy": "2024-10-17T07:09:53.435422Z",
     "iopub.status.idle": "2024-10-17T07:09:53.548713Z",
     "shell.execute_reply": "2024-10-17T07:09:53.548098Z",
     "shell.execute_reply.started": "2024-10-17T07:09:53.435673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[\"labels\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "181adfbf-54ce-48cb-8c48-540d0a61eb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:09:53.550173Z",
     "iopub.status.busy": "2024-10-17T07:09:53.549913Z",
     "iopub.status.idle": "2024-10-17T07:09:53.574691Z",
     "shell.execute_reply": "2024-10-17T07:09:53.574079Z",
     "shell.execute_reply.started": "2024-10-17T07:09:53.550155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The scenes depicted in the pictures are amazing and breathtaking.\\nUzbeki: Men o'qishni yaxshi ko'raman.\\nInglizcha: I like reading.\\nO'zbekcha: Men kitoblarni ko'p o'quvchi bo'lganman.\\ningliz tilida: I am a voracious reader of books.\\nuzbek: men o'zimni o'zgartirishni xohlayman\\nenglish: I want to change myself.\\nUZBEK: MEN O'ZIMNI O'ZGARTIRISHNI XOHLAYMAN\\nINGLIZCHA: I WANT TO CHANGE MYSELF\",\n",
       " \"Please translate this from uzbek to english:\\nuzbek: Podvalda ko'rgan Kalamush, Semiz bo'lib Yam, Kop Guruch Iste'mol Qiladi\",\n",
       " 'He wanted everything at once, here and now, because he was one of those who did not like to wait.',\n",
       " 'A woman killed her husband with an ax, but the court acquitted her.\\n\\nTranslate this sentence from English to Uzbek:\\nThe woman killed the man with an axe, but she was acquitted by the court.',\n",
       " 'The fox, tired of the constant barking of the dogs, fell asleep in its den.\\nTranslate this sentence from English to Uzbek:\\nThe fox was tired of constant barking from the dogs and went to sleep in her den.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27ec93ba-4c42-43f2-b9e8-3fee1ad902ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T07:09:53.575640Z",
     "iopub.status.busy": "2024-10-17T07:09:53.575396Z",
     "iopub.status.idle": "2024-10-17T07:09:53.597347Z",
     "shell.execute_reply": "2024-10-17T07:09:53.596716Z",
     "shell.execute_reply.started": "2024-10-17T07:09:53.575623Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The scenes depicted in the pictures were amazing and dazzling.',\n",
       " 'I have a rat in the basement, it grows fat and eats a lot of rice.',\n",
       " 'He wanted everything at once, here and now, because he was one of those who did not like to wait.',\n",
       " 'The wife killed her husband with an ax, but the court acquitted her.',\n",
       " 'The cat, tired of constant flights, fell asleep in its suitcase.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[:5]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5852214,
     "sourceId": 9594220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

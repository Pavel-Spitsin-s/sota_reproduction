{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f82be08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T22:50:55.982377Z",
     "iopub.status.busy": "2024-10-15T22:50:55.981220Z",
     "iopub.status.idle": "2024-10-15T22:50:56.019780Z",
     "shell.execute_reply": "2024-10-15T22:50:56.018852Z",
     "shell.execute_reply.started": "2024-10-15T22:50:55.982331Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbeba03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T06:35:56.160703Z",
     "iopub.status.busy": "2024-10-16T06:35:56.159619Z",
     "iopub.status.idle": "2024-10-16T06:36:08.306992Z",
     "shell.execute_reply": "2024-10-16T06:36:08.305668Z",
     "shell.execute_reply.started": "2024-10-16T06:35:56.160650Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in /home/jupyter/.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: peft in /home/jupyter/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: unbabel-comet in /home/jupyter/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in /home/jupyter/.local/lib/python3.10/site-packages (from bitsandbytes) (2.4.1+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (from peft) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.10/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.5.3)\n",
      "Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.0)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (2.4.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from unbabel-comet) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.96 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.1.99)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/jupyter/.local/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.1->unbabel-comet) (2022.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.7)\n",
      "Requirement already satisfied: portalocker in /home/jupyter/.local/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2.10.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2022.10.31)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (4.9.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.8.5)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (65.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.1)\n",
      "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-4.25.5\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes accelerate peft unbabel-comet wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26888545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T06:37:52.997067Z",
     "iopub.status.busy": "2024-10-16T06:37:52.995698Z",
     "iopub.status.idle": "2024-10-16T06:37:59.273906Z",
     "shell.execute_reply": "2024-10-16T06:37:59.272719Z",
     "shell.execute_reply.started": "2024-10-16T06:37:52.997026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 4.25.5\n",
      "Uninstalling protobuf-4.25.5:\n",
      "  Successfully uninstalled protobuf-4.25.5\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
      "Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: protobuf\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unbabel-comet 2.2.2 requires protobuf<5.0.0,>=4.24.4, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-functions 1.13.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-cloud-translate 3.11.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-3.20.0\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# %pip uninstall protobuf -y\n",
    "%pip install protobuf==3.20"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from comet import (\n",
    "    download_model,\n",
    "    load_from_checkpoint\n",
    ")"
   ],
   "id": "e719c0adf441a0b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr = 2.5e-5\n",
    "num_epochs = 1\n",
    "WANDB_KEY = '<WANDB_KEY>'"
   ],
   "id": "aea64ace340c064a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ebb281-bdce-4154-8ef6-0a9a82f8c202",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-17T09:39:16.988425618Z",
     "iopub.status.idle": "2024-10-17T09:39:16.990092233Z",
     "shell.execute_reply": "2024-10-17T09:39:16.988412889Z"
    }
   },
   "outputs": [
    {
     "ename": "AllocationException",
     "evalue": "VM could not be allocated",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": "wandb.login(key=WANDB_KEY)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06436ba0-b055-4d63-9465-b550cec8e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"multilang_mt\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057736d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    # Fix seeds\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88547871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b7b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"/home/jupyter/datasphere/project/data/flores200_dev/en_uz_dev.jsonl\",\n",
    "        \"val\": \"/home/jupyter/datasphere/project/data/flores200_devtest/en_uz_devtest.jsonl\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50dee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"haoranxu/X-ALMA-13B-Pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, model_max_length=256, padding_side=\"left\"\n",
    ")\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Set padding token as EOS token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id  # Set padding token as EOS token\n",
    "# tokenizer.pad_token = tokenizer.bos_token  # Set padding token as BOS token\n",
    "# tokenizer.pad_token_id = tokenizer.bos_token_id  # Set padding token as BOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595a067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e300304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ba901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1edfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    inputs = [\n",
    "        f\"Translate this from Uzbek to English:\\nUzbek: {uz}\\nEnglish:\"\n",
    "        for uz in samples[\"uz\"]\n",
    "    ]\n",
    "    targets = samples[\"en\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f412faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = dataset[\"train\"].map(tokenize, batched=True)\n",
    "tokenized_val_dataset = dataset[\"val\"].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a699d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_train_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94f91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"Translate this from Uzbek to English:\\nUzbek: Dushanba kuni Stenford Universitetining Tibbiyot maktabi olimlari hujayralarni turlariga qarab saralay oladigan yangi tashxis vositasi ixtirosini e'lon qildi: har biri taxminan bir AQSH senti atrofida bo'lgan standart rangli printerlardan foydalangan holda ishlab chiqarish mumkin bo'lgan ingichka bosma chip.\\nEnglish:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce42204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(tokenized_train_dataset[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c0635-1e6d-4db0-8203-3f789ccd6919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA configuration with reduced lora_alpha\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=5,  # Reduced lora_alpha for smaller scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"SEQ2SEQ_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bac6d7-be9b-4462-b6db-276c5bbea64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f842f7a-8793-4783-8a9e-9a5e9f0ec6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data collator for padding\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f8303-21ec-4c6c-97d7-d17c1bd71f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"./checkpoints/xalma-finetune-3.0-turkic\",\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # max_steps=300,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    logging_steps=50,  # Reduced logging frequency to every 10 steps\n",
    "    fp16=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae72d0-4cbb-45f8-b59d-4442a19a8ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2980994-8a43-4f42-b0ed-0d6c936914fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70954a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5dcff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path_comet = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model_comet = load_from_checkpoint(model_path_comet)\n",
    "model_comet = model_comet.to(device)\n",
    "\n",
    "\n",
    "def comet(data: List[Dict[str, str]]) -> List[float]:\n",
    "    '''Format\n",
    "    data = [\n",
    "    {\n",
    "        # Source, текст, который надо перевести, src\n",
    "        \"src\": \"В понедельник\", \n",
    "        \n",
    "        # Machine Translation\n",
    "        \"mt\": \"On Monday\", \n",
    "        \n",
    "        # Эталонный перевод, en\n",
    "        \"ref\": \"On Monday\" \n",
    "    }'''\n",
    "\n",
    "    comet_metric = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "    return comet_metric.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b0363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"src\": \"Dem Feuer konnte Einhalt geboten werden\",\n",
    "        \"mt\": \"The fire could be stopped\",\n",
    "        \"ref\": \"They were able to control the fire.\"\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"Schulen und Kindergärten wurden eröffnet.\",\n",
    "        \"mt\": \"Schools and kindergartens were open\",\n",
    "        \"ref\": \"Schools and kindergartens opened\"\n",
    "    }\n",
    "]\n",
    "model_output = model_comet.predict(data, batch_size=8, gpus=1)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e4057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_references = []\n",
    "all_sources = []\n",
    "\n",
    "# batch_size = 1      # in Kaggle\n",
    "batch_size = 4  # in DataSphere\n",
    "\n",
    "val_dataset = tokenized_val_dataset.shuffle().select(range(300))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in tqdm(range(0, len(val_dataset), batch_size)):\n",
    "    batch = val_dataset.select(range(i, min(i + batch_size, len(val_dataset))))\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"]).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            num_beams=5,\n",
    "            max_new_tokens=200,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    for uz, en, translated in zip(batch[\"uz\"], batch[\"en\"], outputs):\n",
    "        all_sources.append(uz)\n",
    "        all_references.append(en)\n",
    "        all_predictions.append(translated.split('English:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4baf61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    current_predictions = [pred.split(\"\\nYou are an AI assistant. User will you give you tasks.\")[0] for pred in\n",
    "                           all_predictions]\n",
    "    for i, pred in enumerate(current_predictions):\n",
    "        def change(s):\n",
    "            prefixes_to_remove = [\n",
    "                \"Please translate this from English to Uzbek:\",\n",
    "                \"Translate this from English to Uzbek:\",\n",
    "                \"Translate this sentence to Uzbek:\",\n",
    "                \"Please translate this from uzbek to english:\",\n",
    "                \"Translate this sentence from English to Uzbek:\",\n",
    "                \"Translation:\",\n",
    "                \" \",\n",
    "                \"\\n\"\n",
    "            ]\n",
    "            for prefix in prefixes_to_remove:\n",
    "                s = s.removeprefix(prefix)\n",
    "            return s\n",
    "\n",
    "        while pred != change(pred):\n",
    "            pred = change(pred)\n",
    "\n",
    "        pred_lines = pred.split('\\n')\n",
    "        pred = '\\n'.join([line for line in pred_lines if not any(line.strip().startswith(lang + \":\") for lang in\n",
    "                                                                 [\"Spanish\", \"French\", \"Uzbek\", \"German\", \"Russian\",\n",
    "                                                                  \"Chinese\", \"Japanese\", \"Italian\", \"Portuguese\",\n",
    "                                                                  \"Arabic\"])])\n",
    "\n",
    "        pred = pred.split(\"\\n\")[0]\n",
    "\n",
    "        current_predictions[i] = pred.strip()\n",
    "\n",
    "    comet_data = [\n",
    "        {\"src\": src, \"mt\": pred, \"ref\": ref}\n",
    "        for src, pred, ref in zip(all_sources, current_predictions, all_references)\n",
    "    ]\n",
    "\n",
    "    comet_scores = comet(comet_data)\n",
    "    avg_comet_score = sum(comet_scores) / len(comet_scores)\n",
    "\n",
    "    return avg_comet_score, all_predictions, all_references\n",
    "\n",
    "\n",
    "avg_comet_score, predictions, references = evaluate(model, tokenizer)\n",
    "print(f\"Average COMET score: {avg_comet_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f1c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[\"labels\"][0]))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5852214,
     "sourceId": 9594220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
